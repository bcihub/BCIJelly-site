"use strict";(self.webpackChunkhcihub_docs=self.webpackChunkhcihub_docs||[]).push([[5244],{8316:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>o,contentTitle:()=>d,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"datasets/invasive/Music","title":"12_Music","description":"Link to the dataset used:","source":"@site/docs/datasets/invasive/12_Music.md","sourceDirName":"datasets/invasive","slug":"/datasets/invasive/Music","permalink":"/BCIJelly-site/docs/next/datasets/invasive/Music","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/datasets/invasive/12_Music.md","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"11_Speech","permalink":"/BCIJelly-site/docs/next/datasets/invasive/Speech"},"next":{"title":"13_Emotion","permalink":"/BCIJelly-site/docs/next/datasets/invasive/Emotion"}}');var n=t(4848),r=t(8453);const i={},d="12_Music",o={},c=[{value:"Link to the dataset used:",id:"link-to-the-dataset-used",level:2},{value:"1.Project Overview",id:"1project-overview",level:2},{value:"2. Preprocessing Process",id:"2-preprocessing-process",level:2},{value:"3. Usage",id:"3-usage",level:2},{value:"3.1 Preparing  Data",id:"31-preparing--data",level:3},{value:"3.2 Running the preprocessed file",id:"32-running-the-preprocessed-file",level:3},{value:"3.3 Results Preview",id:"33-results-preview",level:3},{value:"3.4 Load the preprocessed data",id:"34-load-the-preprocessed-data",level:3}];function l(e){const s={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(s.header,{children:(0,n.jsx)(s.h1,{id:"12_music",children:"12_Music"})}),"\n",(0,n.jsx)(s.h1,{id:"data-overview-and-processing-pipeline",children:"Data Overview and Processing Pipeline"}),"\n",(0,n.jsx)(s.h2,{id:"link-to-the-dataset-used",children:"Link to the dataset used:"}),"\n",(0,n.jsx)(s.p,{children:(0,n.jsx)(s.a,{href:"https://zenodo.org/records/7876019",children:"https://zenodo.org/records/7876019"})}),"\n",(0,n.jsx)(s.h2,{id:"1project-overview",children:"1.Project Overview"}),"\n",(0,n.jsx)(s.p,{children:"This project provides a kind of preprocessing Data from: Nonlinear decoding models enable music reconstruction from human auditory cortex activity dataset in the way that raw signals are transformed into signals for model training. The dataset contains intracranial electroencephalogram (ECoG) data recorded from 29 patients with drug-resistant epilepsy while passively listening to Pink Floyd songs. The dataset contains preprocessed neural activity (high-frequency activity, 70-150 Hz), electrode coordinates (MNI template space), and auditory stimuli (raw waveform files, and auditory spectrograms at 32 and 128 frequency points). The HFA and the two auditory spectrograms were sampled at 100 Hz and temporally aligned (190.72 seconds duration)."}),"\n",(0,n.jsx)(s.h2,{id:"2-preprocessing-process",children:"2. Preprocessing Process"}),"\n",(0,n.jsx)(s.p,{children:"The original intracranial EEG data were read from the.mat format file. By defining the experimental starting point (positive and negative samples), a fixed-length time segment (200 sampling points) was intercepted according to each experiment, and the data feature matrix and the corresponding label file of each patient were saved."}),"\n",(0,n.jsx)(s.h2,{id:"3-usage",children:"3. Usage"}),"\n",(0,n.jsx)(s.h3,{id:"31-preparing--data",children:"3.1 Preparing  Data"}),"\n",(0,n.jsx)(s.p,{children:'Change the "raw_dataset_path" in process.py to the path of the dataset file, and specify the "preprocessed_dataset_path" variable to the path where the preprocessed data will be saved.'}),"\n",(0,n.jsx)(s.h3,{id:"32-running-the-preprocessed-file",children:"3.2 Running the preprocessed file"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-shell",children:"python process.py\n"})}),"\n",(0,n.jsx)(s.p,{children:"Running this file produces a preprocessed (.npy) file with the data stored in preprocessed_dataset_path."}),"\n",(0,n.jsx)(s.h3,{id:"33-results-preview",children:"3.3 Results Preview"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-bash",children:"\u251c\u2500\u2500 preprocessed_dataset_path/\n\u2502   \u251c\u2500\u2500 data_music_reconstruction_preprocessed.npz    # Preprocess the finished data\n"})}),"\n",(0,n.jsx)(s.h3,{id:"34-load-the-preprocessed-data",children:"3.4 Load the preprocessed data"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-python",children:"import numpy as np\n\ndata = np.load('./preprocessed_dataset_path/data_music_reconstruction_preprocessed.npz')\nfeatures = data['spike']  #feature data of shape (number of samples, number of time steps, number of channels)\nlabel = data['label']  # Feature labels, of shape (number of samples, label), labels represent singing or musical events\n\nprint(features.shape, label.shape)\n"})})]})}function p(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,n.jsx)(s,{...e,children:(0,n.jsx)(l,{...e})}):l(e)}},8453:(e,s,t)=>{t.d(s,{R:()=>i,x:()=>d});var a=t(6540);const n={},r=a.createContext(n);function i(e){const s=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function d(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:i(e.components),a.createElement(r.Provider,{value:s},e.children)}}}]);