"use strict";(self.webpackChunkhcihub_docs=self.webpackChunkhcihub_docs||[]).push([[392],{1366:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"datasets/non-invasive/GOD","title":"2_GOD","description":"Generic Object Decoding (fMRI on ImageNet) - OpenNeuro","source":"@site/docs/datasets/non-invasive/2_GOD.md","sourceDirName":"datasets/non-invasive","slug":"/datasets/non-invasive/GOD","permalink":"/BCIJelly-site/docs/next/datasets/non-invasive/GOD","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/datasets/non-invasive/2_GOD.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"1_BOLD5000","permalink":"/BCIJelly-site/docs/next/datasets/non-invasive/BOLD5000"},"next":{"title":"3_EEGMMI","permalink":"/BCIJelly-site/docs/next/datasets/non-invasive/EEGMMI"}}');var a=i(4848),t=i(8453);const r={},l="2_GOD",o={},d=[{value:"1. ROI Signal Extraction (Based on Existing Data and Mask Files)",id:"1-roi-signal-extraction-based-on-existing-data-and-mask-files",level:2},{value:"2. Label Matching (Temporal Alignment)",id:"2-label-matching-temporal-alignment",level:2},{value:"I. Data Preprocessing Workflow",id:"i-data-preprocessing-workflow",level:2},{value:"1. Data Loading and Preparation",id:"1-data-loading-and-preparation",level:3},{value:"2. File Matching and Indexing",id:"2-file-matching-and-indexing",level:3},{value:"3. ROI and Voxel Processing",id:"3-roi-and-voxel-processing",level:3},{value:"4. Trial Data Extraction",id:"4-trial-data-extraction",level:3},{value:"II. Feature Engineering Workflow",id:"ii-feature-engineering-workflow",level:2},{value:"1. Data Standardization",id:"1-data-standardization",level:3},{value:"2. PCA Dimensionality Reduction",id:"2-pca-dimensionality-reduction",level:3},{value:"3. Feature Matrix Construction",id:"3-feature-matrix-construction",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"2_god",children:"2_GOD"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://openneuro.org/datasets/ds001246/versions/1.2.1",children:"Generic Object Decoding (fMRI on ImageNet) - OpenNeuro"})}),"\n",(0,a.jsx)(n.h1,{id:"i-brief-description-of-the-dataset",children:"I. Brief Description of the Dataset"}),"\n",(0,a.jsxs)(n.p,{children:["Based on the requirements of the classification task, we only use the ",(0,a.jsx)(n.code,{children:"perceptionTraining"})," subset (150 categories \xd7 8 images = 1,200 natural images). This subset has undergone standard fMRI preprocessing in the original paper, including head motion correction, spatial registration, and global normalization, making it suitable for subsequent training-test splitting."]}),"\n",(0,a.jsx)(n.h1,{id:"ii-data-processing-ideas",children:"II. Data Processing Ideas"}),"\n",(0,a.jsx)(n.h2,{id:"1-roi-signal-extraction-based-on-existing-data-and-mask-files",children:"1. ROI Signal Extraction (Based on Existing Data and Mask Files)"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Input"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Preprocessed 4-D fMRI time series (",(0,a.jsx)(n.code,{children:"func.nii.gz"}),") located in the ",(0,a.jsx)(n.code,{children:"derivatives"})," folder"]}),"\n",(0,a.jsxs)(n.li,{children:["Predefined ROI masks (",(0,a.jsx)(n.code,{children:"V1.nii.gz"}),", ",(0,a.jsx)(n.code,{children:"LOC.nii.gz"}),", ",(0,a.jsx)(n.code,{children:"FFA.nii.gz"}),", ",(0,a.jsx)(n.code,{children:"PPA.nii.gz"}),", etc., binary masks)"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Workflow"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import nibabel as nib, numpy as np\nimg   = nib.load('func.nii.gz').get_fdata()      # (X,Y,Z,T)\nmask  = nib.load('V1.nii.gz').get_fdata().astype(bool)\nroi_t = img[mask, :]                             # (N_voxel, T)\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Output"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["2D matrix for each ROI: ",(0,a.jsx)(n.code,{children:"roi_t"})," (number of voxels \xd7 number of TRs)."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"2-label-matching-temporal-alignment",children:"2. Label Matching (Temporal Alignment)"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Alignment Logic"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"fMRI sampling period (TR) = 3 s; HRF peak lag \u2248 4\u20136 s."}),"\n",(0,a.jsxs)(n.li,{children:["The signal of the corresponding TR after ",(0,a.jsx)(n.strong,{children:"shifting the stimulus onset time backward by 1 TR (3 s)"})," is taken as the stimulus sample."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Label Table"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["CSV format: ",(0,a.jsx)(n.code,{children:"onset, category_index"})]}),"\n",(0,a.jsx)(n.li,{children:"A one-to-one mapping is established between onset times and TR indices."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h1,{id:"iii-complete-fmri-data-processing-workflow",children:"III. Complete fMRI Data Processing Workflow"}),"\n",(0,a.jsx)(n.h2,{id:"i-data-preprocessing-workflow",children:"I. Data Preprocessing Workflow"}),"\n",(0,a.jsx)(n.h3,{id:"1-data-loading-and-preparation",children:"1. Data Loading and Preparation"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Raw Data Format"}),": fMRI data is stored in ",(0,a.jsx)(n.code,{children:".nii.gz"})," format, event markers in ",(0,a.jsx)(n.code,{children:".tsv"})," format, and ROI masks in ",(0,a.jsx)(n.code,{children:".nii.gz"})," format."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Organization"}),": Structured hierarchically by subject, session, and run."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Key Parameters"}),":","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"TR (Repetition Time): 3.0 seconds"}),"\n",(0,a.jsx)(n.li,{children:"Stimulus Duration: 9.0 seconds"}),"\n",(0,a.jsx)(n.li,{children:"Number of TRs per trial: 3 (9.0/3.0)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-file-matching-and-indexing",children:"2. File Matching and Indexing"}),"\n",(0,a.jsx)(n.p,{children:"Files of different types are matched using regular expressions:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["fMRI data files: ",(0,a.jsx)(n.code,{children:"(sub-\\d+)_ses-(perceptionTraining\\d+)_task-perception_run-(\\d+)_bold_preproc\\.nii\\.gz"})]}),"\n",(0,a.jsxs)(n.li,{children:["Event marker files: ",(0,a.jsx)(n.code,{children:"(sub-\\d+)_ses-(perceptionTraining\\d+)_task-perception_run-(\\d+)_events\\.tsv"})]}),"\n",(0,a.jsxs)(n.li,{children:["ROI mask files: ",(0,a.jsx)(n.code,{children:"(sub-\\d+)_mask_(.+)\\.nii\\.gz"})]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"A data index dictionary is constructed to map subjects to their corresponding data files."}),"\n",(0,a.jsx)(n.h3,{id:"3-roi-and-voxel-processing",children:"3. ROI and Voxel Processing"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Load ROI masks for each subject and perform binarization."}),"\n",(0,a.jsx)(n.li,{children:"Count the number of voxels in each ROI and calculate the total number of voxels."}),"\n",(0,a.jsx)(n.li,{children:"Save ROI names and their corresponding voxel counts in a fixed order to ensure data consistency."}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"4-trial-data-extraction",children:"4. Trial Data Extraction"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Load fMRI data (shape: [x, y, z, t]) and event files."}),"\n",(0,a.jsxs)(n.li,{children:["Determine the TR index for each trial based on the ",(0,a.jsx)(n.code,{children:"onset"})," time in the event file."]}),"\n",(0,a.jsxs)(n.li,{children:["Extract voxel signals of all ROIs for each trial:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Concatenate voxel signals in ROI order."}),"\n",(0,a.jsx)(n.li,{children:"Form trial data with the shape [total voxels, TRs_per_trial]."}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:"Encode trial labels into zero-starting integer codes uniformly."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"ii-feature-engineering-workflow",children:"II. Feature Engineering Workflow"}),"\n",(0,a.jsx)(n.h3,{id:"1-data-standardization",children:"1. Data Standardization"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Standardize data using ",(0,a.jsx)(n.code,{children:"StandardScaler"}),"."]}),"\n",(0,a.jsx)(n.li,{children:"Processing target: All trial data at each time point (TR)."}),"\n",(0,a.jsx)(n.li,{children:"Standardization goal: Mean = 0, Standard deviation = 1."}),"\n",(0,a.jsx)(n.li,{children:"Shape change: [Trials, Total voxels] \u2192 [Trials, Total voxels]"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-pca-dimensionality-reduction",children:"2. PCA Dimensionality Reduction"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Perform Principal Component Analysis using ",(0,a.jsx)(n.code,{children:"sklearn.decomposition.PCA"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Core parameter: ",(0,a.jsx)(n.code,{children:"n_components"})," (number of principal components, adjustable as needed)."]}),"\n",(0,a.jsx)(n.li,{children:"Dimensionality reduction strategy: Process each time point (TR) separately."}),"\n",(0,a.jsx)(n.li,{children:"Shape change: [Trials, Total voxels] \u2192 [Trials, Number of principal components]"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-feature-matrix-construction",children:"3. Feature Matrix Construction"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Integrate dimensionality reduction results from all time points."}),"\n",(0,a.jsxs)(n.li,{children:["Final feature matrix shape: ",(0,a.jsx)(n.code,{children:"(Number of samples \xd7 Number of principal components \xd7 Number of TRs)"}),"."]}),"\n",(0,a.jsx)(n.li,{children:"Retain PCA model parameters (component matrix and explained variance ratio)."}),"\n"]}),"\n",(0,a.jsx)(n.h1,{id:"iv-usage-instructions",children:"IV. Usage Instructions"}),"\n",(0,a.jsx)(n.p,{children:"Data is stored per subject."}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Load ",(0,a.jsx)(n.code,{children:".npz"})," files directly using the ",(0,a.jsx)(n.code,{children:"np.load"})," function."]}),"\n",(0,a.jsxs)(n.li,{children:["After loading, the following keywords can be used to access corresponding data:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"fmri"}),": fMRI data with shape ",(0,a.jsx)(n.code,{children:"(trials, number of principal components, TRs)"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"label"}),": Label data containing 150 values (0\u2013149), each corresponding to a different category."]}),"\n",(0,a.jsxs)(n.li,{children:["Additional extended keywords:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"label_names"}),": Obtain specific names corresponding to labels."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"rois"}),": Obtain information about Regions of Interest (ROIs)."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"roi_voxel_counts"}),": Obtain the number of voxels in each ROI."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h1,{id:"notes",children:"Notes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["The ",(0,a.jsx)(n.code,{children:"n_components"})," parameter can be adjusted according to actual needs to balance the dimensionality reduction effect and computational resource consumption. The data uploaded in this study has been reduced to 1000 dimensions, which is the setting that achieved the optimal classification performance in the current experiments."]}),"\n",(0,a.jsx)(n.li,{children:"For data augmentation, category-specific processing was attempted, but experimental results showed that the adopted data augmentation methods did not improve the classification performance, so they were not included in the final workflow."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var s=i(6540);const a={},t=s.createContext(a);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);