"use strict";(self.webpackChunkhcihub_docs=self.webpackChunkhcihub_docs||[]).push([[8286],{3187:(n,e,s)=>{s.r(e),s.d(e,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"datasets/non-invasive/SEED-VII","title":"5_SEED-VII","description":"Dataset Link","source":"@site/docs/datasets/non-invasive/5_SEED-VII.md","sourceDirName":"datasets/non-invasive","slug":"/datasets/non-invasive/SEED-VII","permalink":"/BCIJelly-site/docs/next/datasets/non-invasive/SEED-VII","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/datasets/non-invasive/5_SEED-VII.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"4_SEED IV","permalink":"/BCIJelly-site/docs/next/datasets/non-invasive/SEED IV"},"next":{"title":"6_Things EEG","permalink":"/BCIJelly-site/docs/next/datasets/non-invasive/Things EEG"}}');var l=s(4848),t=s(8453);const a={},r="5_SEED-VII",o={},d=[{value:"Dataset Link",id:"dataset-link",level:2},{value:"1. Experimental Design &amp; Data Collection",id:"1-experimental-design--data-collection",level:2},{value:"Participants",id:"participants",level:3},{value:"Sessions &amp; Trials",id:"sessions--trials",level:3},{value:"Emotion Induction Design",id:"emotion-induction-design",level:3},{value:"2. Data Preprocessing Workflow",id:"2-data-preprocessing-workflow",level:2},{value:"EEG Signals",id:"eeg-signals",level:3},{value:"3. Labels &amp; Synchronization",id:"3-labels--synchronization",level:2},{value:"4. Example Data Loading",id:"4-example-data-loading",level:2},{value:"5. Feature Data Description",id:"5-feature-data-description",level:2}];function c(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(e.header,{children:(0,l.jsx)(e.h1,{id:"5_seed-vii",children:"5_SEED-VII"})}),"\n",(0,l.jsx)(e.h1,{id:"introduction--processing-workflow",children:"Introduction & Processing Workflow"}),"\n",(0,l.jsx)(e.h2,{id:"dataset-link",children:"Dataset Link"}),"\n",(0,l.jsx)(e.p,{children:(0,l.jsx)(e.a,{href:"https://bcmi.sjtu.edu.cn/~seed/seed-vii.html",children:"https://bcmi.sjtu.edu.cn/~seed/seed-vii.html"})}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"1-experimental-design--data-collection",children:"1. Experimental Design & Data Collection"}),"\n",(0,l.jsx)(e.h3,{id:"participants",children:"Participants"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Number"}),": 20 participants (10 male, 10 female), aged 19\u201326 years (mean: 22.5, STD: 1.8)."]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Condition"}),": All right-handed, with normal or corrected-to-normal vision and normal hearing."]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Selection"}),": Screened using the Eysenck Personality Questionnaire (EPQ)."]}),"\n"]}),"\n",(0,l.jsx)(e.h3,{id:"sessions--trials",children:"Sessions & Trials"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsxs)(e.li,{children:["Each participant completed ",(0,l.jsx)(e.strong,{children:"4 sessions"})," (on different days)."]}),"\n",(0,l.jsxs)(e.li,{children:["Each session contained ",(0,l.jsx)(e.strong,{children:"20 trials"}),":","\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Part 1"}),": Watching video clips for emotion induction (duration: 2\u20135 minutes)."]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Part 2"}),": Self-assessment of emotional state (score range: 0\u20131)."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(e.h3,{id:"emotion-induction-design",children:"Emotion Induction Design"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsxs)(e.li,{children:["A total of ",(0,l.jsx)(e.strong,{children:"80 video clips"}),":","\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Emotions: Happiness, Sadness, Fear, Anger, Disgust, Surprise, Neutral (Neutral has only 8 clips)."}),"\n",(0,l.jsx)(e.li,{children:"Each session contained only 5 out of the 7 emotions to avoid excessive emotional switching."}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(e.li,{children:"The video sequence was carefully arranged to avoid sudden emotional changes."}),"\n"]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"2-data-preprocessing-workflow",children:"2. Data Preprocessing Workflow"}),"\n",(0,l.jsx)(e.h3,{id:"eeg-signals",children:"EEG Signals"}),"\n",(0,l.jsxs)(e.ol,{children:["\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Bad Channel Inspection & Interpolation"}),"\nVisually inspect EEG signals using MNE-Python, and interpolate any bad channels."]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Filtering"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Bandpass filter: 0.1\u201370 Hz"}),"\n",(0,l.jsx)(e.li,{children:"Notch filter: 50 Hz (to remove powerline interference)"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Downsampling"}),"\nFrom the original 1000 Hz sampling rate to 200 Hz."]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Segmentation & Feature Extraction"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsx)(e.li,{children:"Window size: Non-overlapping 4-second segments"}),"\n",(0,l.jsxs)(e.li,{children:["Frequency bands:","\n",(0,l.jsxs)(e.ol,{children:["\n",(0,l.jsx)(e.li,{children:"\u03b4 (1\u20134 Hz)"}),"\n",(0,l.jsx)(e.li,{children:"\u03b8 (4\u20138 Hz)"}),"\n",(0,l.jsx)(e.li,{children:"\u03b1 (8\u201314 Hz)"}),"\n",(0,l.jsx)(e.li,{children:"\u03b2 (14\u201331 Hz)"}),"\n",(0,l.jsx)(e.li,{children:"\u03b3 (31\u201350 Hz)"}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(e.li,{children:"Feature: Differential Entropy (DE)"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Feature Smoothing"}),"\nApply Linear Dynamic System (LDS) smoothing or moving average."]}),"\n"]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"3-labels--synchronization",children:"3. Labels & Synchronization"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Label source"}),": Target emotion category of each video (7 classes) + subjective score (0\u20131)."]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:"Time alignment"}),": EEG/Eye-tracking features are aligned with the video playback timeline, with each 4-second window treated as one sample."]}),"\n"]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"4-example-data-loading",children:"4. Example Data Loading"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:'import os\nimport numpy as np\n\ndef load_data(session_id, sub_id):\n    """\n    Load a .npz data file for a specific session and subject ID.\n\n    Parameters\n    ----------\n    session_id : int, optional, default=1\n        The session number, corresponding to the folder named "session{session_id}".\n    sub_id : int, optional, default=1\n        The subject ID used to filter files starting with "subject{sub_id}".\n  \n    Returns\n    -------\n    numpy.lib.npyio.NpzFile or None\n        The loaded npz file object if found; otherwise None.\n    """\n    for file in os.listdir(f"session{session_id}"):\n        if file.startswith(f\'subject{sub_id}\') and file.endswith(\'.npz\'):\n            print(f"Loading data from: {file} (session={session_id})")\n            return np.load(os.path.join(f"session{session_id}", file), allow_pickle=True)\n'})}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h2,{id:"5-feature-data-description",children:"5. Feature Data Description"}),"\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:(0,l.jsx)(e.code,{children:"features"})}),":","\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsxs)(e.li,{children:["3D array ",(0,l.jsx)(e.code,{children:"[samples, frequency_bands, channels]"})]}),"\n",(0,l.jsx)(e.li,{children:"Frequency bands fixed at 5 (\u03b4, \u03b8, \u03b1, \u03b2, \u03b3)"}),"\n",(0,l.jsx)(e.li,{children:"Channels fixed at 62"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(e.li,{children:[(0,l.jsx)(e.strong,{children:(0,l.jsx)(e.code,{children:"labels"})}),":","\n",(0,l.jsxs)(e.ul,{children:["\n",(0,l.jsxs)(e.li,{children:["1D array ",(0,l.jsx)(e.code,{children:"[samples]"})]}),"\n",(0,l.jsx)(e.li,{children:"Each element is an emotion category label (0\u20136 or 0\u20133, depending on task definition)"}),"\n"]}),"\n"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,l.jsx)(e,{...n,children:(0,l.jsx)(c,{...n})}):c(n)}},8453:(n,e,s)=>{s.d(e,{R:()=>a,x:()=>r});var i=s(6540);const l={},t=i.createContext(l);function a(n){const e=i.useContext(t);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(l):n.components||l:a(n.components),i.createElement(t.Provider,{value:e},n.children)}}}]);