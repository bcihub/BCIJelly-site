"use strict";(self.webpackChunkhcihub_docs=self.webpackChunkhcihub_docs||[]).push([[2017],{8357:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"datasets/non-invasive/BOLD5000","title":"1_BOLD5000","description":"Dataset Link","source":"@site/docs/datasets/non-invasive/1_BOLD5000.md","sourceDirName":"datasets/non-invasive","slug":"/datasets/non-invasive/BOLD5000","permalink":"/BCIJelly-site/docs/next/datasets/non-invasive/BOLD5000","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/datasets/non-invasive/1_BOLD5000.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"13_Emotion","permalink":"/BCIJelly-site/docs/next/datasets/invasive/Emotion"},"next":{"title":"2_GOD","permalink":"/BCIJelly-site/docs/next/datasets/non-invasive/GOD"}}');var r=i(4848),t=i(8453);const a={},l="1_BOLD5000",o={},d=[{value:"Dataset Link",id:"dataset-link",level:2},{value:"1. fMRI Time Series Extraction",id:"1-fmri-time-series-extraction",level:2},{value:"2. <strong>Tags (Based on events.tsv)</strong>",id:"2-tags-based-on-eventstsv",level:2},{value:"3. 1D Time Series to 2D Features (Based on GAF Method)",id:"3-1d-time-series-to-2d-features-based-on-gaf-method",level:2},{value:"Overall Process (Reference [1]):",id:"overall-process-reference-1",level:3},{value:"Core Principles of GAF",id:"core-principles-of-gaf",level:3},{value:"Function Analysis:",id:"function-analysis",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"1_bold5000",children:"1_BOLD5000"})}),"\n",(0,r.jsx)(n.h2,{id:"dataset-link",children:"Dataset Link"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://bold5000-dataset.github.io/website/download.html",children:"BOLD5000"})}),"\n",(0,r.jsx)(n.h1,{id:"i-project-background",children:"I. Project Background"}),"\n",(0,r.jsx)(n.p,{children:"This project aims to conduct standardized preprocessing on the BOLD5000 dataset, extract fMRI features and perform dimensionality transformation, providing high-quality input features and corresponding labels for subsequent model training related to visual cognition (such as image classification, brain activity prediction, etc.)."}),"\n",(0,r.jsx)(n.h1,{id:"ii-data-processing",children:"II. Data Processing"}),"\n",(0,r.jsx)(n.h2,{id:"1-fmri-time-series-extraction",children:"1. fMRI Time Series Extraction"}),"\n",(0,r.jsxs)(n.p,{children:["The time series data of all voxels in the ROIs provided in the dataset release 2.0 version (",(0,r.jsx)(n.a,{href:"https://figshare.com/s/bbaf45dca1b1b873ddfa",children:"BOLD5000 Release 2.0"}),") after GLM analysis are directly used as fMRI features (folder BOLD5000_GLMsingle_ROI_betas)."]}),"\n",(0,r.jsxs)(n.p,{children:["This data has undergone the following process: [See the article ",(0,r.jsx)(n.strong,{children:"BOLD5000"})," A public fMRI dataset of 5000 images]"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Preprocessing"}),"\n",(0,r.jsxs)(n.p,{children:["All the original fMRI data have been processed through the standardized pipeline using ",(0,r.jsx)(n.strong,{children:"fMRIPREP 1.1.4"}),", including:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Motion correction, slice timing correction, geometric distortion correction"}),"\n",(0,r.jsx)(n.li,{children:"Functional-anatomical registration, MNI152NLin2009cAsym spatial normalization"}),"\n",(0,r.jsx)(n.li,{children:"Regression of 9 nuisance variables (6 motion parameters + CSF/WM/global signal)"}),"\n",(0,r.jsx)(n.li,{children:"128 s high-pass filtering"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Functional Localizer Analysis"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"On 8 (or 6) localizer runs, use SPM12 to establish a GLM and run the following contrasts:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Scene > Object & Scrambled \u2192",(0,r.jsx)(n.strong,{children:"PPA, RSC, OPA"})]}),"\n",(0,r.jsxs)(n.li,{children:["Object > Scrambled \u2192 ",(0,r.jsx)(n.strong,{children:"LOC"})]}),"\n",(0,r.jsxs)(n.li,{children:["Scrambled > Baseline \u2192 ",(0,r.jsx)(n.strong,{children:"EarlyVis"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"FWE correction (p < 0.0001, k \u2265 30) was applied to the significant voxels to obtain individual spatial binary ROI masks, which were then resampled to the MNI voxel grid."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"The time window was averaged (based on the peak activation of ROI signals)."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"CSI1-3: average of TR3 + TR4 (4 - 8 s)"}),"\n",(0,r.jsx)(n.li,{children:"CSI4: only TR3 (4 - 6 s)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Signal extraction: the time series of all voxels within each ROI, demeaned."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Main Experiment Single Trial GLM Modeling"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"On the preprocessed main experimental data, a GLM-single-trial (LS-S) model was established for each stimulus image one by one:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Each stimulus image corresponds to a regressor (stick function, 1 s ON, 9 s OFF)."}),"\n",(0,r.jsx)(n.li,{children:"The same 9 nuisance regressors, run regressors, and 128 s high-pass filtering were again included."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"For each voxel within the ROI mask, the \u03b2 coefficient (event-related amplitude) of this image at this voxel was solved."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.h2,{id:"2-tags-based-on-eventstsv",children:["2. ",(0,r.jsx)(n.strong,{children:"Tags (Based on events.tsv)"})]}),"\n",(0,r.jsx)(n.p,{children:"The labels can be directly read from the event.csv file in the dataset."}),"\n",(0,r.jsxs)(n.p,{children:["Each fMRI run has a corresponding ",(0,r.jsx)(n.code,{children:"events.tsv"})," file, with fields including:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"onset"}),": The time point when the image appears."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"duration"}),": Usually 1 second (the time the image is presented)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"trial_type"}),": Fixed as ",(0,r.jsx)(n.code,{children:"stimulus"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"stimulus"}),": The file name of the image."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"ImgID"}),": The unique ID of the image (used for pairing image features)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"ImgType"}),": The type of the image (ImageNet, coco, scenes)."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The tag processing method is as follows:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Extract the image type labels (ImgType) from the event.csv file and perform standardization processing. Standardization refers to classifying the rep category (repeated occurrences) and non-rep into one category."}),"\n",(0,r.jsx)(n.li,{children:"Convert the string format labels into numerical format acceptable by machine learning models."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"3-1d-time-series-to-2d-features-based-on-gaf-method",children:"3. 1D Time Series to 2D Features (Based on GAF Method)"}),"\n",(0,r.jsx)(n.h3,{id:"overall-process-reference-1",children:"Overall Process (Reference [1]):"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time Series Extraction"}),"\nComplete time series are extracted from each fMRI run and each active voxel, representing the distribution of BOLD signal intensity over time. As each run contains 37 visual stimuli (images), the length of the time series is 37."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Time Series Segmentation"}),"\nThe complete time series is segmented into three parts based on the dataset (COCO, ImageNet, SUN) to which the images presented in each trial belong, corresponding to the BOLD signals during the viewing of the three types of images."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Preprocessing"}),"\nLinear interpolation is applied to the voxel-specific time series after segmentation to uniformly adjust them to the same time length, in order to meet the requirements of subsequent analysis."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Conversion from 1D Time Series to 2D Spatial Features"})}),"\n",(0,r.jsx)(n.p,{children:"The 1D time series is transformed into a 2D matrix through Gramian Angular Field (GAF) to capture the spatial correlation features within the time series."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"core-principles-of-gaf",children:"Core Principles of GAF"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Gramian Angular Field (GAF)"}),": Through polar coordinate transformation, the normalized time series is mapped to a 2D matrix to generate Gramian Angular Summation Field (GASF) and Gramian Angular Difference Field (GADF), preserving time dependency without information loss."]}),"\n",(0,r.jsx)(n.p,{children:"Specific Steps"}),"\n",(0,r.jsx)(n.p,{children:"(1) Data Normalization"}),"\n",(0,r.jsxs)(n.p,{children:["Normalize a 1D time series (with length ",(0,r.jsx)(n.code,{children:"T"}),") to the range ",(0,r.jsx)(n.code,{children:"[-1, 1]"}),". The formula is:"]}),"\n",(0,r.jsx)(n.p,{children:"Run Python"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"x_norm = 2 * (x - min(x)) / (max(x) - min(x)) - 1\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Among them, ",(0,r.jsx)(n.code,{children:"x"})," represents the original time series, and ",(0,r.jsx)(n.code,{children:"x_norm"})," is the normalized result."]}),"\n",(0,r.jsx)(n.p,{children:"(2) Polar Coordinate Transformation"}),"\n",(0,r.jsxs)(n.p,{children:["Map the normalized sequence to polar coordinates ",(0,r.jsx)(n.code,{children:"(r, \u03b8)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Angle ",(0,r.jsx)(n.code,{children:"\u03b8"}),": Calculated from ",(0,r.jsx)(n.code,{children:"x_norm"})," as ",(0,r.jsx)(n.code,{children:"\u03b8 = arccos(x_norm)"})," (since ",(0,r.jsx)(n.code,{children:"x_norm \u2208 [-1, 1]"}),", ",(0,r.jsx)(n.code,{children:"\u03b8 \u2208 [0, \u03c0]"}),");"]}),"\n",(0,r.jsxs)(n.li,{children:["Radius ",(0,r.jsx)(n.code,{children:"r"}),": Normalized from the time step, ",(0,r.jsx)(n.code,{children:"r = t / T"})," (",(0,r.jsx)(n.code,{children:"t"})," is the time index, ",(0,r.jsx)(n.code,{children:"t \u2208 [0, T-1]"}),")."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"(3) Calculation of GASF and GADF"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GASF"}),": ",(0,r.jsx)(n.code,{children:"GASF[i, j] = cos(\u03b8_i + \u03b8_j)"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GADF"}),": ",(0,r.jsx)(n.code,{children:"GADF[i, j] = sin(\u03b8_i - \u03b8_j)"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Here, ",(0,r.jsx)(n.code,{children:"i"})," and ",(0,r.jsx)(n.code,{children:"j"})," are matrix indices, and ",(0,r.jsx)(n.code,{children:"\u03b8_i"})," and ",(0,r.jsx)(n.code,{children:"\u03b8_j"})," are the angles at the ",(0,r.jsx)(n.code,{children:"i"}),"th and ",(0,r.jsx)(n.code,{children:"j"}),"th time steps, respectively."]}),"\n",(0,r.jsx)(n.h1,{id:"iii-main-function-descriptions",children:"III. Main Function Descriptions"}),"\n",(0,r.jsxs)(n.p,{children:["The main function of the ",(0,r.jsx)(n.code,{children:"mark_img"})," function is to convert time series data into image data suitable for input into a convolutional neural network (CNN), as detailed below:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def mark_img(dat):\n\tgram = GramianAngularField(image_size=16, method='summation')##change method='difference' for GramianAngularField difference\n\tgram_t = gram.fit_transform(dat.iloc[:,:-1])\n\tx = gram_t.reshape(gram_t.shape[0], 16, 16, 1) y=dat['label']\n\treturn x,y\n"})}),"\n",(0,r.jsx)(n.h3,{id:"function-analysis",children:"Function Analysis:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Core Tools for Converting Time Series to Images"}),"\nThe ",(0,r.jsx)(n.code,{children:"GramianAngularField"})," algorithm in the ",(0,r.jsx)(n.code,{children:"pyts.image"})," library is used to convert time series data into images. This is a common technique for handling time series data, which can preserve the temporal correlation and amplitude information of the time series."]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Parameter Description"})}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"GramianAngularField(image_size=16, method='summation')"}),": Generates a 16x16 image, with the conversion method set to ",(0,r.jsx)(n.code,{children:"summation"}),". It can also be changed to ",(0,r.jsx)(n.code,{children:"difference"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"dat"}),": The input DataFrame, where the last column is the label (",(0,r.jsx)(n.code,{children:"label"}),"), and the remaining columns are the time series feature data."]}),"\n",(0,r.jsxs)(n.li,{children:["Parameter ",(0,r.jsx)(n.code,{children:"16"})," is specified for the article, mainly based on the experimental design corresponding to the dataset itself."]}),"\n"]}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Data Processing Steps"})}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Extract the time series features from the input data except for the last column (the label column) as ",(0,r.jsx)(n.code,{children:"dat.iloc[:,:-1]"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Obtain the image data ",(0,r.jsx)(n.code,{children:"gram_t"})," through Gramian Angular Field transformation, with a shape of ",(0,r.jsx)(n.code,{children:"(number of samples, 16, 16)"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Reshape the image data to the input format required by CNN, which is ",(0,r.jsx)(n.code,{children:"(number of samples, 16, 16, 1)"})," (the last dimension represents the number of channels, and here it is a single-channel grayscale image)."]}),"\n",(0,r.jsxs)(n.li,{children:["Extract the label column ",(0,r.jsx)(n.code,{children:"dat['label']"})," as the target variable ",(0,r.jsx)(n.code,{children:"y"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.ol,{start:"4",children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Return Values"})}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"x"}),": The transformed image data (shape: ",(0,r.jsx)(n.code,{children:"(n_samples, 16, 16, 1)"}),")."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"y"}),": The corresponding label data, used as the target value for model training."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This function realizes the conversion from one-dimensional time series to two-dimensional images, preparing for the subsequent deep learning classification tasks."}),"\n",(0,r.jsx)(n.h1,{id:"iv-instructions-for-use",children:"IV. Instructions for Use"}),"\n",(0,r.jsx)(n.p,{children:"Data is stored on a per-subject basis (supporting direct training with data from individual subjects)."}),"\n",(0,r.jsx)(n.p,{children:"Usage method:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Directly load the ",(0,r.jsx)(n.code,{children:".npz"})," format file through the ",(0,r.jsx)(n.code,{children:"np.load"})," function."]}),"\n",(0,r.jsxs)(n.li,{children:["After loading, the corresponding data can be obtained through the two keywords ",(0,r.jsx)(n.code,{children:"fmri"})," and ",(0,r.jsx)(n.code,{children:"label"}),":"]}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"fmri"}),": fMRI two-dimensional image data, with a shape of ",(0,r.jsx)(n.code,{children:"(trials, height, weight)"}),", where both height and weight are 16."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"label"}),": Label data, containing three values: 0, 1, and 2, which respectively correspond to:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"0: coco category"}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"1: imagenet category"}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"2: scenes category"}),"\n",(0,r.jsx)(n.p,{children:"The ratio of the three types of labels is 2:2:1."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h1,{id:"v-notes-to-observe",children:"V. Notes to Observe"}),"\n",(0,r.jsx)(n.p,{children:"The current data is based on the signals of all voxels within the ROIs (regions of interest) (refer to Article [1]). In practical analysis, in addition to using the signals of all voxels, it is also possible to consider performing dimensionality reduction on each ROI separately before conducting subsequent analysis."}),"\n",(0,r.jsx)(n.h1,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:'V. K. Kancharala, D. Bhattacharya and N. Sinha, "Spatial Encoding of BOLD fMRI Time Series for Categorizing Static Images Across Visual Datasets: A Pilot Study on Human Vision," TENCON 2023 - 2023 IEEE Region 10 Conference (TENCON), Chiang Mai, Thailand, 2023, pp. 1117-1122, doi: 10.1109/TENCON58879.2023.10322476.'}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>l});var s=i(6540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);