"use strict";(self.webpackChunkhcihub_docs=self.webpackChunkhcihub_docs||[]).push([[6348],{8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>d});var t=s(6540);const i={},a=t.createContext(i);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(a.Provider,{value:n},e.children)}},9359:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"datasets/invasive/Speech","title":"11_Speech","description":"Link to the dataset used:","source":"@site/docs/datasets/invasive/11_Speech.md","sourceDirName":"datasets/invasive","slug":"/datasets/invasive/Speech","permalink":"/BCIJelly-site/docs/next/datasets/invasive/Speech","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/datasets/invasive/11_Speech.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"10_Visual Grating Task","permalink":"/BCIJelly-site/docs/next/datasets/invasive/Visual Grating Task"},"next":{"title":"12_Music","permalink":"/BCIJelly-site/docs/next/datasets/invasive/Music"}}');var i=s(4848),a=s(8453);const r={},d="11_Speech",o={},l=[{value:"Link to the dataset used:",id:"link-to-the-dataset-used",level:2},{value:"1. Project Overview",id:"1-project-overview",level:2},{value:"2.  Preprocessing Process",id:"2--preprocessing-process",level:2},{value:"3.  Core Functions",id:"3--core-functions",level:2},{value:"3.1 loadFeaturesAndNormalize()",id:"31-loadfeaturesandnormalize",level:4},{value:"3.2 getDataset()",id:"32-getdataset",level:4},{value:"4. Usage",id:"4-usage",level:2},{value:"4.1 Preparing  Data",id:"41-preparing--data",level:3},{value:"4.2 Running the preprocessed file",id:"42-running-the-preprocessed-file",level:3},{value:"4.3 Results Preview",id:"43-results-preview",level:3},{value:"4.4 Load the preprocessed data",id:"44-load-the-preprocessed-data",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"11_speech",children:"11_Speech"})}),"\n",(0,i.jsx)(n.h1,{id:"neuroprosthesis-data-overview-and-processing-pipeline",children:"Neuroprosthesis Data Overview and Processing Pipeline"}),"\n",(0,i.jsx)(n.h2,{id:"link-to-the-dataset-used",children:"Link to the dataset used:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://datadryad.org/dataset/doi:10.5061/dryad.x69p8czpq",children:"https://datadryad.org/dataset/doi:10.5061/dryad.x69p8czpq"})}),"\n",(0,i.jsx)(n.h2,{id:"1-project-overview",children:"1. Project Overview"}),"\n",(0,i.jsx)(n.p,{children:'This project provides A way to preprocess the Data for: A high-performance speech neuroprosthesis dataset to transform raw signals into signals for model training. The dataset consists of 12,100 sentences spoken by a single study participant. We use the "competitionData" version of it, which is a simplified version of the sentence data formatted and partitioned for machine learning research. It contains "train" and "test" partitions for model development, and a reserved "competitionHoldOut" set for the speech decoding competition (labels will be released after the competition). We currently only use the train (8800) versus test (880) partition data. For each sentence, a transcript of what the participant was trying to say was provided, along with a corresponding time series of neural spike activity recorded from 256 microelectrodes in speech related areas of the cortex.'}),"\n",(0,i.jsx)(n.h2,{id:"2--preprocessing-process",children:"2.  Preprocessing Process"}),"\n",(0,i.jsx)(n.p,{children:"The pulse signal data was read from the.mat format file for block standardization operation, and the corresponding sentence list and the corresponding time step number of each sentence were read.\nThe neural signal data processed in the previous step are combined with their corresponding sentence transcriptions to generate a structured dataset for language decoding model training (e.g. converting brain signals into text). The key tasks include: alignment of neural signal features with phoneme sequences; Text cleaning, phoneme conversion, sequence normalization; Generate input formats for the model (fixed-length sequences, length statistics, etc.)"}),"\n",(0,i.jsx)(n.h2,{id:"3--core-functions",children:"3.  Core Functions"}),"\n",(0,i.jsx)(n.h4,{id:"31-loadfeaturesandnormalize",children:"3.1 loadFeaturesAndNormalize()"}),"\n",(0,i.jsx)(n.p,{children:"loadFeaturesAndNormalize(sessionPath)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Neural signal data are loaded and block normalized, and finally the processed features and corresponding text transcriptions are returned"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Inputs"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sessionPath"}),"\uff1a",(0,i.jsx)(n.code,{children:'str = ""'}),"\uff0cPath to the loaded subject file (.mat)."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"outputs"}),"\uff1a","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"session_data"}),"\uff1a",(0,i.jsx)(n.code,{children:"dict{ 'inputFeatures': input_features, 'transcriptions': transcriptions, 'frameLens': frame_lens }"}),", containing the following fields:"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"inputFeatures"}),"\uff1a",(0,i.jsx)(n.code,{children:"List[np.ndarray]"}),"\uff0c A normalized list of neural signal features with each element of shape (time_steps, channel)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"transcriptions"}),"\uff1a",(0,i.jsx)(n.code,{children:"List[str]"}),"\uff0cThe corresponding text list (with leading and trailing Spaces removed)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"frameLens"}),"\uff1a",(0,i.jsx)(n.code,{children:"List[int]"}),"\uff0cTime step (number of frames) for each Trial."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"32-getdataset",children:"3.2 getDataset()"}),"\n",(0,i.jsx)(n.p,{children:"getDataset(sessionPath)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),"\uff1a Data processing pipeline, which is used to convert neural signal data with corresponding text transcriptions into a format suitable for decoding model training."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Inputs"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sessionPath"}),"\uff1a",(0,i.jsx)(n.code,{children:'str = ""'}),"\uff0cContains neural signals and text data (.mat) file path."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"outputs"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"newDataset"}),"\uff1a",(0,i.jsx)(n.code,{children:"dict{ 'sentenceDat': sentenceDat, 'transcriptions': transcriptions, 'phonemes': phonemes, 'timeSeriesLens': timeSeriesLens, 'phoneLens': phoneLens, 'phonePerTime': phonePerTime, }"}),"\uff0ccontaining the following fields:"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sentenceDat"}),"\uff1a",(0,i.jsx)(n.code,{children:"List[np.ndarray]"}),"\uff0clist of neural signal features, each of shape (time_steps, channel)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"transcriptions"}),"\uff1a",(0,i.jsx)(n.code,{children:"List[str]"}),"\uff0cThe corresponding text list with leading and trailing whitespace removed."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"phonemes"}),"\uff1a",(0,i.jsx)(n.code,{children:"List[np.ndarray]"}),"\uff0cEncoded phoneme sequence, fixed length 500, padded with zeros."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"timeSeriesLens"}),"\uff1a",(0,i.jsx)(n.code,{children:"np.ndarray"}),"\uff0cThe time step for each sentence."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"phoneLens"}),"\uff1a",(0,i.jsx)(n.code,{children:"np.ndarray"}),"\uff0c The actual phoneme length of each sentence (without padding)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"phonePerTime"}),"\uff1a",(0,i.jsx)(n.code,{children:"np.ndarray"}),"\uff0c The ratio of the number of phonemes to the length of the time series (for alignment)."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"4-usage",children:"4. Usage"}),"\n",(0,i.jsx)(n.h3,{id:"41-preparing--data",children:"4.1 Preparing  Data"}),"\n",(0,i.jsx)(n.p,{children:'Modify the path of "dataDir" in the process.py file to be the path of the "competitionData" file, and specify the value of "out_data" variable in the file to be the path where the preprocessed data will be saved.'}),"\n",(0,i.jsx)(n.h3,{id:"42-running-the-preprocessed-file",children:"4.2 Running the preprocessed file"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"python process.py\n"})}),"\n",(0,i.jsx)(n.p,{children:"Run this file to get the preprocessed (.npy) file."}),"\n",(0,i.jsx)(n.h3,{id:"43-results-preview",children:"4.3 Results Preview"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u251c\u2500\u2500 out_data/\n\u2502   \u251c\u2500\u2500 data_SpeechBCI_preprocessed.npz          # Preprocess the finished data\n"})}),"\n",(0,i.jsx)(n.h3,{id:"44-load-the-preprocessed-data",children:"4.4 Load the preprocessed data"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import numpy as np\n\ndata = np.load('./out_data/data_SpeechBCI_preprocessed.npz')\nfeatures = data['spike']      #feature data of shape (number of samples, number of channels, number of time steps)\nlabel = data['label']         #Feature label of shape (number of samples, label), label represents the encoded phoneme sequence, fixed length 500.\nfeatures_len = data['X_len']  #time step of feature data of shape (number of samples)\nlabel_len = data['y_len']     #actual phoneme length (without padding) for each sentence in shape (number of samples)\n\n\nprint(features.shape, label.shape, features_len.shape, label_len.shape)\n"})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);